{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "12c989a2272087144a907f3af46e789b70a90abf7fd5b4372cac90cccd9eaa13"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 3. 使用CNN进行文本分类 \n",
    "\n",
    "<img src='img/textcnn.jfif' width=500>\n",
    "\n",
    "reference:\n",
    "- https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "- https://github.com/649453932/Chinese-Text-Classification-Pytorch/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Namespace(batch_size=16, learning_rate=0.004, max_length=2000, dropout=0.2, embedding_dim=200, hidden_dim=200, seed=42, num_filters=200, filter_sizes=[1, 2, 3], num_train_epochs=20, model_save_path='data/save_model/textcnn.path')"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "hparams = argparse.Namespace(**{\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 0.004,\n",
    "    # 'max_grad_norm': 1.,\n",
    "    'max_length': 2000,\n",
    "    'dropout': 0.2,\n",
    "    'embedding_dim': 200,\n",
    "    'hidden_dim': 200,\n",
    "    'seed': 42,\n",
    "    'num_filters': 200,\n",
    "    'filter_sizes': [1, 2, 3],\n",
    "    'num_train_epochs': 20,\n",
    "    'model_save_path': 'data/save_model/textcnn.path',\n",
    "})\n",
    "\n",
    "hparams"
   ]
  },
  {
   "source": [
    "### 加载数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train: 1600\ntest: 400\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "random.seed(hparams.seed)\n",
    "\n",
    "\n",
    "def load_movie_reviews():\n",
    "    pos_ids = movie_reviews.fileids('pos')\n",
    "    neg_ids = movie_reviews.fileids('neg')\n",
    "\n",
    "    all_reviews = []\n",
    "    for pids in pos_ids:\n",
    "        all_reviews.append((movie_reviews.raw(pids), 'positive'))\n",
    "    \n",
    "    for nids in neg_ids:\n",
    "        all_reviews.append((movie_reviews.raw(nids), 'negative'))\n",
    "\n",
    "    random.shuffle(all_reviews)\n",
    "    train_reviews = all_reviews[:1600]\n",
    "    test_reviews = all_reviews[1600:]\n",
    "\n",
    "    return train_reviews, test_reviews\n",
    "\n",
    "train_reviews, test_reviews = load_movie_reviews()\n",
    "print('train:', len(train_reviews))\n",
    "print('test:', len(test_reviews))"
   ]
  },
  {
   "source": [
    "### Tokenize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "train_reviews_tokenized = []\n",
    "train_labels = []\n",
    "\n",
    "for review, label in train_reviews:\n",
    "    label = 0 if label == 'negative' else 1\n",
    "    tokenized = word_tokenize(review)\n",
    "\n",
    "    train_labels.append(label)\n",
    "    train_reviews_tokenized.append(tokenized)\n",
    "\n",
    "\n",
    "test_reviews_tokenized = []\n",
    "test_labels = []\n",
    "\n",
    "for review, label in test_reviews:\n",
    "    label = 0 if label == 'negative' else 1\n",
    "    tokenized = word_tokenize(review)\n",
    "\n",
    "    test_labels.append(label)\n",
    "    test_reviews_tokenized.append(tokenized)"
   ]
  },
  {
   "source": [
    "### 建立词表、将单词变成id"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "42013\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "\n",
    "counter = Counter()\n",
    "for review in train_reviews_tokenized:# + test_reviews_tokenized:\n",
    "    counter.update(review)\n",
    "\n",
    "vocab = Vocab(counter, min_freq=1, specials=['<unk>', '<pad>', '<sos>', '<eos>'])\n",
    "\n",
    "hparams.vocab_size = len(vocab)\n",
    "hparams.pad_id = vocab['<pad>']\n",
    "hparams.num_classes = 2\n",
    "\n",
    "print(hparams.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews_ids = [vocab.lookup_indices(review) for review in train_reviews_tokenized]\n",
    "test_reviews_ids = [vocab.lookup_indices(review) for review in test_reviews_tokenized]"
   ]
  },
  {
   "source": [
    "### 将数据打包为dataloader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.reviews[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "\n",
    "def collate_to_max_length(batch):\n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "    for X, y in batch:\n",
    "        if len(X) >= hparams.max_length:\n",
    "            X = X[:hparams.max_length]\n",
    "        else:\n",
    "            X = X + [hparams.pad_id] * (hparams.max_length-len(X))\n",
    "\n",
    "        X_batch.append(X)\n",
    "        y_batch.append(y)\n",
    "\n",
    "    return torch.tensor(X_batch), torch.tensor(y_batch)\n",
    "\n",
    "\n",
    "train_dataset = TextDataset(train_reviews_ids, train_labels)\n",
    "test_dataset = TextDataset(test_reviews_ids, test_labels)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=hparams.batch_size, \n",
    "    collate_fn=collate_to_max_length, \n",
    "    shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=hparams.batch_size,\n",
    "    collate_fn=collate_to_max_length,\n",
    "    shuffle=False)"
   ]
  },
  {
   "source": [
    "### 定义模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams    \n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            hparams.vocab_size, \n",
    "            hparams.embedding_dim, \n",
    "            padding_idx=hparams.pad_id)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, hparams.num_filters, (k, hparams.embedding_dim))\n",
    "            for k in hparams.filter_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(hparams.dropout)\n",
    "\n",
    "        hidden_size = hparams.num_filters * len(hparams.filter_sizes)\n",
    "        self.classifier = nn.Linear(hidden_size, hparams.num_classes)\n",
    "            \n",
    "        # self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, w in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                # w.data.xavier_normal_()\n",
    "                nn.init.xavier_normal_(w)\n",
    "            elif 'bias' in name:\n",
    "                w.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [B, L, embedding_dim]\n",
    "        embed = self.embedding(x)\n",
    "        # [B, 1, L, embedding_dim]\n",
    "        embed = embed.unsqueeze(1)\n",
    "        \n",
    "        # [(B, num_filters), ...] => [(B, num_filters*len(filter_sizes))]\n",
    "        hidden = torch.cat([self.conv_and_pool(embed, conv) for conv in self.convs], dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        logits = self.classifier(hidden)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        # (B, 1, L, embedding_dim) => (B, 1, L, 1, num_filters)\n",
    "        # (B, 1, L, 1, num_filters) => (B, 1, L, num_filters)\n",
    "        x = F.relu(conv(x).squeeze(3))\n",
    "        # (B, 1, L, num_filters) => (B, 1, num_filters)\n",
    "        # (B, 1, num_filters) => (B, num_filters)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab.load_vectors('glove.6B.200d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TextCNN(\n",
       "  (embedding): Embedding(42013, 200, padding_idx=1)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 200, kernel_size=(1, 200), stride=(1, 1))\n",
       "    (1): Conv2d(1, 200, kernel_size=(2, 200), stride=(1, 1))\n",
       "    (2): Conv2d(1, 200, kernel_size=(3, 200), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (classifier): Linear(in_features=600, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "model = TextCNN(hparams)\n",
    "\n",
    "# model.embedding.weight.data.copy_(vocab.vectors)\n",
    "# model.embedding.weight.requires_grad = False\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=hparams.learning_rate, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.95**epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, loss_func, optimizer, epoch_idx, hparams):\n",
    "    model.train()\n",
    "    \n",
    "    pbar = tqdm(dataloader)\n",
    "    pbar.set_description(f'Epoch {epoch_idx}')\n",
    "\n",
    "    for X, y in pbar:\n",
    "        if torch.cuda.is_available():\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)  # (B, 2)\n",
    "        loss = loss_func(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, loss_func):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    total_loss = 0.\n",
    "    correct_num = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader)\n",
    "        pbar.set_description('Valid')\n",
    "        for X, y in pbar:\n",
    "            if torch.cuda.is_available():\n",
    "                X = X.cuda()\n",
    "                y = y.cuda()\n",
    "            output = model(X)\n",
    "            \n",
    "            loss = loss_func(output, y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            correct_num = correct_num + (output.argmax(1) == y).float().sum().item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = correct_num / len(dataloader.dataset)\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 1: 100%|██████████| 100/100 [00:12<00:00,  7.84it/s, loss=1.06]\n",
      "Valid: 100%|██████████| 25/25 [00:01<00:00, 24.06it/s]\n",
      "[Validation] loss: 2.4536, accuracy: 0.5050, LR: [0.0038]     \n",
      "save model to data/save_model/textcnn.path\n",
      "\n",
      "\n",
      "Epoch 2: 100%|██████████| 100/100 [00:13<00:00,  7.62it/s, loss=0.582]\n",
      "Valid: 100%|██████████| 25/25 [00:01<00:00, 23.36it/s]\n",
      "[Validation] loss: 0.5370, accuracy: 0.7475, LR: [0.00361]     \n",
      "save model to data/save_model/textcnn.path\n",
      "\n",
      "\n",
      "Epoch 3: 100%|██████████| 100/100 [00:13<00:00,  7.45it/s, loss=2.07]\n",
      "Valid: 100%|██████████| 25/25 [00:01<00:00, 23.80it/s]\n",
      "[Validation] loss: 2.1426, accuracy: 0.5375, LR: [0.0034295]     \n",
      "Epoch 4: 100%|██████████| 100/100 [00:13<00:00,  7.24it/s, loss=0.557]\n",
      "Valid: 100%|██████████| 25/25 [00:01<00:00, 21.25it/s]\n",
      "[Validation] loss: 0.6729, accuracy: 0.7575, LR: [0.0032580249999999995]     \n",
      "Epoch 5: 100%|██████████| 100/100 [00:13<00:00,  7.29it/s, loss=0.168]\n",
      "Valid: 100%|██████████| 25/25 [00:01<00:00, 23.74it/s]\n",
      "[Validation] loss: 1.2081, accuracy: 0.6850, LR: [0.003095123749999999]     \n",
      "Epoch 6: 100%|██████████| 100/100 [00:13<00:00,  7.27it/s, loss=0.156]\n",
      "Valid: 100%|██████████| 25/25 [00:01<00:00, 23.30it/s]\n",
      "[Validation] loss: 0.8599, accuracy: 0.7500, LR: [0.0029403675624999994]     \n",
      "Epoch 7: 100%|██████████| 100/100 [00:14<00:00,  6.98it/s, loss=0.0577]\n",
      "Valid: 100%|██████████| 25/25 [00:01<00:00, 22.15it/s]\n",
      "[Validation] loss: 0.7202, accuracy: 0.7825, LR: [0.002793349184374999]     \n",
      "Epoch 8: 100%|██████████| 100/100 [00:14<00:00,  7.03it/s, loss=0.388]\n",
      "Valid: 100%|██████████| 25/25 [00:01<00:00, 22.26it/s]\n",
      "[Validation] loss: 2.0009, accuracy: 0.5875, LR: [0.002653681725156249]     \n",
      "Epoch 9: 100%|██████████| 100/100 [00:14<00:00,  6.98it/s, loss=0.325]\n",
      "Valid: 100%|██████████| 25/25 [00:01<00:00, 23.14it/s]\n",
      "[Validation] loss: 0.4722, accuracy: 0.8300, LR: [0.0025209976388984364]     \n",
      "save model to data/save_model/textcnn.path\n",
      "\n",
      "\n",
      "Epoch 10: 100%|██████████| 100/100 [00:14<00:00,  7.10it/s, loss=0.0688]\n",
      "Valid: 100%|██████████| 25/25 [00:01<00:00, 23.16it/s]\n",
      "[Validation] loss: 0.4172, accuracy: 0.8450, LR: [0.0023949477569535148]     \n",
      "save model to data/save_model/textcnn.path\n",
      "\n",
      "\n",
      "Epoch 11: 100%|██████████| 100/100 [00:14<00:00,  7.13it/s, loss=0.305]\n",
      "Valid: 100%|██████████| 25/25 [00:01<00:00, 22.70it/s]\n",
      "[Validation] loss: 0.4907, accuracy: 0.8500, LR: [0.0022752003691058386]     \n",
      "Epoch 12: 100%|██████████| 100/100 [00:13<00:00,  7.21it/s, loss=0.145]\n",
      "Valid: 100%|██████████| 25/25 [00:01<00:00, 22.65it/s]\n",
      "[Validation] loss: 0.6339, accuracy: 0.8150, LR: [0.0021614403506505465]     \n",
      "Epoch 13: 100%|██████████| 100/100 [00:14<00:00,  7.06it/s, loss=0.198]\n",
      "Valid: 100%|██████████| 25/25 [00:01<00:00, 22.81it/s]\n",
      "[Validation] loss: 0.5488, accuracy: 0.8275, LR: [0.002053368333118019]     \n",
      "Epoch 14: 100%|██████████| 100/100 [00:13<00:00,  7.20it/s, loss=0.0083]\n",
      "Valid: 100%|██████████| 25/25 [00:01<00:00, 22.94it/s]\n",
      "[Validation] loss: 0.4898, accuracy: 0.8375, LR: [0.0019506999164621182]     \n",
      "Epoch 15: 100%|██████████| 100/100 [00:14<00:00,  7.04it/s, loss=0.0679]\n",
      "Valid: 100%|██████████| 25/25 [00:01<00:00, 22.78it/s]\n",
      "[Validation] loss: 0.4537, accuracy: 0.8475, LR: [0.001853164920639012]     \n",
      "Epoch 16: 100%|██████████| 100/100 [00:13<00:00,  7.15it/s, loss=0.00185]\n",
      "Valid: 100%|██████████| 25/25 [00:01<00:00, 22.98it/s]\n",
      "[Validation] loss: 0.5228, accuracy: 0.8175, LR: [0.0017605066746070614]     \n",
      "Epoch 17: 100%|██████████| 100/100 [00:13<00:00,  7.22it/s, loss=0.145]\n",
      "Valid: 100%|██████████| 25/25 [00:01<00:00, 22.96it/s]\n",
      "[Validation] loss: 0.4460, accuracy: 0.8375, LR: [0.0016724813408767083]     \n",
      "Epoch 18: 100%|██████████| 100/100 [00:14<00:00,  7.12it/s, loss=0.00839]\n",
      "Valid: 100%|██████████| 25/25 [00:01<00:00, 22.85it/s]\n",
      "[Validation] loss: 0.4305, accuracy: 0.8475, LR: [0.0015888572738328728]     \n",
      "Epoch 19: 100%|██████████| 100/100 [00:14<00:00,  7.13it/s, loss=0.00325]\n",
      "Valid: 100%|██████████| 25/25 [00:01<00:00, 22.36it/s]\n",
      "[Validation] loss: 0.4393, accuracy: 0.8275, LR: [0.001509414410141229]     \n",
      "Epoch 20: 100%|██████████| 100/100 [00:14<00:00,  7.13it/s, loss=0.189]\n",
      "[Validation] loss: 0.4367, accuracy: 0.8450, LR: [0.0014339436896341675]     \n",
      "accuracy_at_lowest_loss: 0.845, best_accuracy: 0.85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "accuracy_at_lowest_loss = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch_idx in range(hparams.num_train_epochs):\n",
    "    train(model, train_dataloader, loss_func, optimizer, epoch_idx+1, hparams)\n",
    "    scheduler.step()\n",
    "    val_loss, accuracy = evaluate(model, test_dataloader, loss_func)\n",
    "    best_accuracy = max(best_accuracy, accuracy)\n",
    "    print(f'\\r[Validation] loss: {val_loss:.4f}, accuracy: {accuracy:.4f}, LR: {scheduler.get_last_lr()}     ')\n",
    "\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        torch.save(model.state_dict(), hparams.model_save_path)\n",
    "        print(f'\\rsave model to {hparams.model_save_path}\\n\\n')\n",
    "        best_val_loss = val_loss\n",
    "        accuracy_at_lowest_loss = accuracy\n",
    "\n",
    "print(f'accuracy_at_lowest_loss: {accuracy_at_lowest_loss}, best_accuracy: {best_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "拓展\n",
    "\n",
    "- 如何理解textcnn中的卷积核和pooling层\n",
    "- 如何确定卷积核的大小，调参？\n",
    "    - RCNN[1]\n",
    "\n",
    "[1] Lai, Siwei, et al. \"Recurrent convolutional neural networks for text classification.\" AAAI2015."
   ]
  }
 ]
}