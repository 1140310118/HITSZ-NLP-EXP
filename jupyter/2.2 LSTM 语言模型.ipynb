{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "12c989a2272087144a907f3af46e789b70a90abf7fd5b4372cac90cccd9eaa13"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 2. 使用LSTM进行语言建模\n",
    "\n",
    "<img src='img/rnn-lm.jpg' width=500>\n",
    "\n",
    "reference:\n",
    "- https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "- https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html?highlight=language%20modeling\n",
    "- https://github.com/pytorch/examples/tree/master/word_language_model\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 设置超参数\n",
    "\n",
    "一般，这些超参数是通过 argparse.ArgumentParser 从命令行中获取的。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Namespace(batch_size=16, learning_rate=5, max_grad_norm=1.0, bptt=32, dropout=0.2, embedding_dim=200, hidden_dim=200, n_layers=4, tie_weights=True, seed=42, num_train_epochs=20, lm_data_dir='data/PennTreebank', model_save_path='data/save_model/lstm_lm.path', temperature=1.0)"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "hparams = argparse.Namespace(**{\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 5,\n",
    "    'max_grad_norm': 1.,\n",
    "    'bptt': 32,  # sequence_length\n",
    "    'dropout': 0.2,\n",
    "    'embedding_dim': 200,\n",
    "    'hidden_dim': 200,\n",
    "    'n_layers': 4,\n",
    "    'tie_weights': True,\n",
    "    'seed': 42,\n",
    "    'num_train_epochs': 20,\n",
    "    'lm_data_dir': 'data/PennTreebank',\n",
    "    'model_save_path': 'data/save_model/lstm_lm.path',\n",
    "    'temperature': 1.\n",
    "})\n",
    "\n",
    "hparams"
   ]
  },
  {
   "source": [
    "### 加载数据\n",
    "\n",
    "- 加载数据\n",
    "- tokenize\n",
    "- 建立词表，并将词映射为id\n",
    "- 将数据打包到batch中"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1. Tokenization\n",
      "---------------------\n",
      "train: tokenizing ...\n",
      "val: tokenizing ...\n",
      "test: tokenizing ...\n",
      "Done!\n",
      "\n",
      "\n",
      "2. Build Vocab\n",
      "Done!\n",
      "\n",
      "\n",
      "3. To ids\n",
      "Done!\n",
      "\n",
      "\n",
      "4. batchify\n",
      "Done!\n",
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9924"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from PennTreebankCorpus import Corpus\n",
    "\n",
    "\n",
    "corpus = Corpus(root_dir=hparams.lm_data_dir)\n",
    "corpus.load_datasets()\n",
    "train_data, val_data, test_data = corpus.preprocess(hparams.batch_size)\n",
    "\n",
    "hparams.vocab_size = corpus.vocab_size()\n",
    "\n",
    "hparams.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([60659, 16])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "train_data.size()\n",
    "# 加载词向量"
   ]
  },
  {
   "source": [
    "### 建立模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams    \n",
    "\n",
    "        self.drop = nn.Dropout(hparams.dropout)\n",
    "        self.embedding = nn.Embedding(hparams.vocab_size, hparams.embedding_dim)\n",
    "        self.rnn_layer = nn.LSTM(\n",
    "            hparams.embedding_dim,\n",
    "            hparams.hidden_dim, \n",
    "            hparams.n_layers,\n",
    "            dropout=hparams.dropout)\n",
    "\n",
    "        self.decoder = nn.Linear(hparams.hidden_dim, hparams.vocab_size)\n",
    "        if hparams.tie_weights:\n",
    "            assert hparams.embedding_dim == hparams.hidden_dim\n",
    "            self.decoder.weight = self.embedding.weight\n",
    "\n",
    "        # 参数初始化\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range = 0.1\n",
    "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        # (L, B) => (L, B, embedding_dim)\n",
    "        emb = self.embedding(input)        \n",
    "        emb = self.drop(emb)\n",
    "\n",
    "        # emb (L, B, embedding_dim) => output (L, B, hidden_dim)\n",
    "        output, hidden = self.rnn_layer(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        \n",
    "        # (L, B, hidden_dim) => (L, B, vocab_size)\n",
    "        # 为每个位置预测下个词\n",
    "        decoded = self.decoder(output)\n",
    "        return decoded, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        初始化 第一个隐状态和细胞状态\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.hparams.n_layers, batch_size, self.hparams.hidden_dim),\n",
    "                weight.new_zeros(self.hparams.n_layers, batch_size, self.hparams.hidden_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (drop): Dropout(p=0.2, inplace=False)\n",
       "  (embedding): Embedding(9924, 200)\n",
       "  (rnn_layer): LSTM(200, 200, num_layers=4, dropout=0.2)\n",
       "  (decoder): Linear(in_features=200, out_features=9924, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "model = LSTMModel(hparams)\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=hparams.learning_rate, momentum=0.9)"
   ]
  },
  {
   "source": [
    "### 训练"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i, hparams):\n",
    "    seq_len = min(hparams.bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len]\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "def train(model, train_data, loss_func, optimizer, epoch_idx, hparams):\n",
    "    model.train()\n",
    "    hidden = model.init_hidden(hparams.batch_size)\n",
    "    \n",
    "    pbar = tqdm(range(0, train_data.size(0)-1, hparams.bptt))\n",
    "    pbar.set_description(f'Epoch {epoch_idx}')\n",
    "\n",
    "    \n",
    "    for i in pbar:\n",
    "        # left_context: (L, B)\n",
    "        # target: (L, B)\n",
    "        left_context, targets = get_batch(train_data, i, hparams)\n",
    "\n",
    "        left_context = left_context.cuda()\n",
    "        targets = targets.cuda()\n",
    "\n",
    "        output, hidden = model(left_context, hidden)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_func(output.view(-1, hparams.vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # 梯度裁剪，防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), hparams.max_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        # 混淆度：评估语言模型的一个指标\n",
    "        ppl = math.exp(loss.item())\n",
    "        pbar.set_postfix(loss=loss.item(), ppl=ppl)\n",
    "\n",
    "        # 中断梯度\n",
    "        hidden = repackage_hidden(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_val_data, loss_func, hparams):\n",
    "    model.eval()\n",
    "    hidden = model.init_hidden(hparams.batch_size)\n",
    "    total_loss = 0.\n",
    "\n",
    "    hidden = model.init_hidden(hparams.batch_size)\n",
    "    with torch.no_grad():\n",
    "        n_steps = 0\n",
    "        pbar = tqdm(range(0, test_val_data.size(0)-1, hparams.bptt))\n",
    "        pbar.set_description('Valid')\n",
    "        for i in pbar:\n",
    "            left_context, targets = get_batch(test_val_data, i, hparams)\n",
    "            \n",
    "            left_context = left_context.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            output, hidden = model(left_context, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            loss = loss_func(output.view(-1, hparams.vocab_size), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            n_steps += 1\n",
    "    avg_loss = total_loss / n_steps\n",
    "    ppl = math.exp(avg_loss)\n",
    "    return avg_loss, ppl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 1: 100%|██████████| 1896/1896 [01:24<00:00, 22.36it/s, loss=5.27, ppl=195]\n",
      "Valid: 100%|██████████| 151/151 [00:01<00:00, 86.48it/s]\n",
      "[Validation] loss: 5.0366, PPL: 154, LR: 5                       \n",
      "save model to data/save_model/lstm_lm.path\n",
      "\n",
      "Epoch 2: 100%|██████████| 1896/1896 [01:19<00:00, 23.82it/s, loss=4.97, ppl=144]\n",
      "Valid: 100%|██████████| 151/151 [00:01<00:00, 84.41it/s]\n",
      "[Validation] loss: 4.8388, PPL: 126, LR: 5                       \n",
      "save model to data/save_model/lstm_lm.path\n",
      "\n",
      "Epoch 3: 100%|██████████| 1896/1896 [01:23<00:00, 22.62it/s, loss=4.92, ppl=137]\n",
      "Valid: 100%|██████████| 151/151 [00:01<00:00, 77.41it/s]\n",
      "[Validation] loss: 4.7688, PPL: 118, LR: 5                       \n",
      "save model to data/save_model/lstm_lm.path\n",
      "\n",
      "Epoch 4: 100%|██████████| 1896/1896 [01:32<00:00, 20.54it/s, loss=4.84, ppl=126]\n",
      "Valid: 100%|██████████| 151/151 [00:01<00:00, 76.74it/s]\n",
      "[Validation] loss: 4.7285, PPL: 113, LR: 5                       \n",
      "save model to data/save_model/lstm_lm.path\n",
      "\n",
      "Epoch 5: 100%|██████████| 1896/1896 [01:29<00:00, 21.11it/s, loss=4.81, ppl=123]\n",
      "Valid: 100%|██████████| 151/151 [00:02<00:00, 70.38it/s]\n",
      "[Validation] loss: 4.7203, PPL: 112, LR: 5                       \n",
      "save model to data/save_model/lstm_lm.path\n",
      "\n",
      "Epoch 6: 100%|██████████| 1896/1896 [01:30<00:00, 21.02it/s, loss=4.9, ppl=134]\n",
      "Valid: 100%|██████████| 151/151 [00:01<00:00, 78.86it/s]\n",
      "[Validation] loss: 4.7106, PPL: 111, LR: 5                       \n",
      "save model to data/save_model/lstm_lm.path\n",
      "\n",
      "Epoch 7: 100%|██████████| 1896/1896 [01:27<00:00, 21.73it/s, loss=4.96, ppl=142]\n",
      "Valid: 100%|██████████| 151/151 [00:01<00:00, 79.69it/s]\n",
      "[Validation] loss: 4.7307, PPL: 113, LR: 5                       \n",
      "Epoch 8: 100%|██████████| 1896/1896 [01:28<00:00, 21.49it/s, loss=4.78, ppl=120]\n",
      "Valid: 100%|██████████| 151/151 [00:01<00:00, 80.69it/s]\n",
      "[Validation] loss: 4.5550, PPL: 95, LR: 1.25                       \n",
      "save model to data/save_model/lstm_lm.path\n",
      "\n",
      "Epoch 9: 100%|██████████| 1896/1896 [01:28<00:00, 21.33it/s, loss=4.59, ppl=98.3]\n",
      "Valid: 100%|██████████| 151/151 [00:01<00:00, 78.45it/s]\n",
      "[Validation] loss: 4.5282, PPL: 93, LR: 1.25                       \n",
      "save model to data/save_model/lstm_lm.path\n",
      "\n",
      "Epoch 10: 100%|██████████| 1896/1896 [01:37<00:00, 19.36it/s, loss=4.52, ppl=91.6]\n",
      "Valid: 100%|██████████| 151/151 [00:01<00:00, 76.86it/s]\n",
      "[Validation] loss: 4.5027, PPL: 90, LR: 1.25                       \n",
      "save model to data/save_model/lstm_lm.path\n",
      "\n",
      "Epoch 11: 100%|██████████| 1896/1896 [01:29<00:00, 21.30it/s, loss=4.55, ppl=95]\n",
      "Valid: 100%|██████████| 151/151 [00:01<00:00, 79.29it/s]\n",
      "[Validation] loss: 4.4879, PPL: 89, LR: 1.25                       \n",
      "save model to data/save_model/lstm_lm.path\n",
      "\n",
      "Epoch 12: 100%|██████████| 1896/1896 [01:27<00:00, 21.76it/s, loss=4.46, ppl=86.2]\n",
      "Valid: 100%|██████████| 151/151 [00:01<00:00, 80.21it/s]\n",
      "[Validation] loss: 4.4733, PPL: 88, LR: 1.25                       \n",
      "save model to data/save_model/lstm_lm.path\n",
      "\n",
      "Epoch 13: 100%|██████████| 1896/1896 [11:52:07<00:00, 22.54s/it, loss=4.47, ppl=87.6]\n",
      "Valid: 100%|██████████| 151/151 [00:01<00:00, 78.12it/s]\n",
      "[Validation] loss: 4.4596, PPL: 86, LR: 1.25                       \n",
      "save model to data/save_model/lstm_lm.path\n",
      "\n",
      "Epoch 14: 100%|██████████| 1896/1896 [01:21<00:00, 23.17it/s, loss=4.5, ppl=90]\n",
      "Valid: 100%|██████████| 151/151 [00:01<00:00, 81.61it/s]\n",
      "[Validation] loss: 4.4485, PPL: 85, LR: 1.25                       \n",
      "save model to data/save_model/lstm_lm.path\n",
      "\n",
      "Epoch 15: 100%|██████████| 1896/1896 [01:25<00:00, 22.05it/s, loss=4.39, ppl=80.6]\n",
      "Valid: 100%|██████████| 151/151 [00:02<00:00, 63.80it/s]\n",
      "[Validation] loss: 4.4389, PPL: 85, LR: 1.25                       \n",
      "save model to data/save_model/lstm_lm.path\n",
      "\n",
      "Epoch 16: 100%|██████████| 1896/1896 [02:22<00:00, 13.33it/s, loss=4.34, ppl=77]\n",
      "Valid: 100%|██████████| 151/151 [00:02<00:00, 51.05it/s]\n",
      "[Validation] loss: 4.4312, PPL: 84, LR: 1.25                       \n",
      "save model to data/save_model/lstm_lm.path\n",
      "\n",
      "Epoch 17: 100%|██████████| 1896/1896 [01:55<00:00, 16.38it/s, loss=4.38, ppl=79.5]\n",
      "Valid: 100%|██████████| 151/151 [00:02<00:00, 74.20it/s]\n",
      "[Validation] loss: 4.4241, PPL: 83, LR: 1.25                       \n",
      "save model to data/save_model/lstm_lm.path\n",
      "\n",
      "Epoch 18: 100%|██████████| 1896/1896 [01:40<00:00, 18.90it/s, loss=4.32, ppl=75.2]\n",
      "Valid: 100%|██████████| 151/151 [00:01<00:00, 84.68it/s]\n",
      "[Validation] loss: 4.4169, PPL: 83, LR: 1.25                       \n",
      "save model to data/save_model/lstm_lm.path\n",
      "\n",
      "Epoch 19: 100%|██████████| 1896/1896 [01:25<00:00, 22.16it/s, loss=4.36, ppl=78.6]\n",
      "Valid: 100%|██████████| 151/151 [00:01<00:00, 89.22it/s]\n",
      "[Validation] loss: 4.4074, PPL: 82, LR: 1.25                       \n",
      "save model to data/save_model/lstm_lm.path\n",
      "\n",
      "Epoch 20: 100%|██████████| 1896/1896 [01:17<00:00, 24.48it/s, loss=4.31, ppl=74.7]\n",
      "[Validation] loss: 4.4024, PPL: 82, LR: 1.25                       \n",
      "save model to data/save_model/lstm_lm.path\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "learning_rate = hparams.learning_rate\n",
    "\n",
    "for epoch_idx in range(hparams.num_train_epochs):\n",
    "    train(model, train_data, loss_func, optimizer, epoch_idx+1, hparams)\n",
    "    val_loss, ppl = evaluate(model, val_data, loss_func, hparams)\n",
    "    print(f'\\r[Validation] loss: {val_loss:.4f}, PPL: {ppl:.0f}, LR: {learning_rate}                       ')\n",
    "    \n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        torch.save(model.state_dict(), hparams.model_save_path)\n",
    "        print(F'save model to {hparams.model_save_path}\\n')\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            learning_rate /= 4\n",
    "            param_group['lr'] = learning_rate\n"
   ]
  },
  {
   "source": [
    "加载模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (drop): Dropout(p=0.2, inplace=False)\n",
       "  (embedding): Embedding(9924, 200)\n",
       "  (rnn_layer): LSTM(200, 200, num_layers=4, dropout=0.2)\n",
       "  (decoder): Linear(in_features=200, out_features=9924, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "model = LSTMModel(hparams)\n",
    "model.load_state_dict(torch.load(hparams.model_save_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[next_word] candidates: in and to of on for but that with \n",
      "<sos> we have no useful information on\n",
      "[next_word] candidates: the a their this n its an it our \n",
      "<sos> we have no useful information on this\n",
      "[next_word] candidates: year of time country and week market month \n",
      "<sos> we have no useful information on this \n",
      "[next_word] candidates: and of in the or to a but \n",
      "<sos> we have no useful information on this  but\n",
      "[next_word] candidates: it the they we i he that a in \n",
      "<sos> we have no useful information on this  but we\n",
      "[next_word] candidates: ' can do have are need would were ca want \n",
      "<sos> we have no useful information on this  but we do\n",
      "[next_word] candidates: n not in have and to they it \n",
      "<sos> we have no useful information on this  but we do have\n",
      "[next_word] candidates: a to the n been in an no it \n",
      "<sos> we have no useful information on this  but we do have to\n",
      "[next_word] candidates: be do see get have make go sell take \n",
      "<sos> we have no useful information on this  but we do have to see\n",
      "[next_word] candidates: the it that a what to in their \n",
      "<sos> we have no useful information on this  but we do have to see their\n",
      "[next_word] candidates: own money work way position customers people shares ability \n",
      "<sos> we have no useful information on this  but we do have to see their money\n",
      "[next_word] candidates: and to in on for says or of \n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(hparams.seed)\n",
    "torch.cuda.manual_seed(hparams.seed)\n",
    "\n",
    "\n",
    "input_sentence = ['<sos>', 'we', 'have', 'no', 'useful', 'information']\n",
    "\n",
    "input_ = torch.tensor([corpus.vocab[word] for word in input_sentence])\n",
    "input_ = input_.view(-1, 1)\n",
    "\n",
    "hidden = model.init_hidden(1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(20):\n",
    "        # (4, 1, vocab_size)\n",
    "        output, hidden = model(input_, hidden)\n",
    "        # (1, vocab_size)\n",
    "        # 最后一个位置的预测结果\n",
    "        output = output[-1]\n",
    "        # (vocab_size)\n",
    "        word_weights = output.squeeze().div(hparams.temperature).exp()\n",
    "        # (10, )\n",
    "        topk_word = word_weights.topk(k=10)[1]\n",
    "        topk_word = [word_idx for word_idx in topk_word if word_idx not in (corpus.vocab['<eos>'], corpus.vocab['<unk>'])]\n",
    "\n",
    "        print('[next_word] candidates:', end=' ')\n",
    "        for word_idx in topk_word:\n",
    "            word = corpus.vocab.itos[word_idx]\n",
    "            print(f'{word}', end=' ')\n",
    "        print()\n",
    "\n",
    "        input_word = input()\n",
    "        if input_word == '.':\n",
    "            break\n",
    "        word_idx = corpus.vocab[input_word]\n",
    "        input_sentence.append(input_word)\n",
    "        print(' '.join(input_sentence))\n",
    "        # word_idx = topk_word[0]\n",
    "        input_= torch.tensor([word_idx]).view(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<sos> <eos> \n",
      "<sos> it has ruled that no magazine saying might draw new professional for any part to big insurance <eos> \n",
      "<sos> a white house spokesman called the rules of congress with appropriations safety and milk system by gte ' s market <eos> \n",
      "<sos> such losses are to likely said stop estimated the japanese sides <eos> \n",
      "<sos> the increasing start in recent months are n ' t expected to join the united aid program <eos> \n",
      "<sos> congress was n ' t likely to kill approved general government confidence in the wake of a federal bank <eos> \n",
      "<sos> the new jersey meeting "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(hparams.seed)\n",
    "torch.cuda.manual_seed(hparams.seed)\n",
    "\n",
    "\n",
    "# input_ = torch.randint(hparams.vocab_size, (1, 1), dtype=torch.long)\n",
    "\n",
    "input_ = torch.tensor(corpus.vocab['<sos>'])\n",
    "input_ = input_.view(-1, 1)\n",
    "hidden = model.init_hidden(1)\n",
    "\n",
    "word = corpus.vocab.itos[input_.item()]\n",
    "print(word, end=' ')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(100):\n",
    "        output, hidden = model(input_, hidden)\n",
    "\n",
    "        word_weights = output.squeeze().div(1).exp()\n",
    "        word_weights[corpus.vocab['<unk>']] = 0.\n",
    "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "\n",
    "        input_.fill_(word_idx)\n",
    "        word = corpus.vocab.itos[word_idx]\n",
    "        print(word, end=' ')\n",
    "        if word == '<eos>':\n",
    "            print()"
   ]
  },
  {
   "source": [
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 拓展\n",
    "\n",
    "1. LSTM得到的句子表示本身就可以用于下游任务\n",
    "1. LSTM的并行性不够友好，难以用于大规模语料上的训练 => Transformer\n",
    "2. 文本生成的解码策略：Greedy Search vs Beam Search\n",
    "\n",
    "refernece: https://huggingface.co/blog/how-to-generate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}